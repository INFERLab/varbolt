{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Bernoulli Variational Autoencoder for Energy Disaggregation\n",
    "## (a.k.a. variational BOLT)\n",
    "The corresponding paper is under review at the moment. But here is the code for anyone who wants to play with it.\n",
    "It's completely unsupervised and all you would have to do is feed data into it.\n",
    "\n",
    "Important: This code runs only with keras 2.0.1. Because InputSpecs were changes in later versions, it won't run with a newer version of keras than 2.0.1.\n",
    "\n",
    "In order to speed up computations, in a pre-processing step, events are extracted from aggregate power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import cluster\n",
    "import pickle\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, LSTM, TimeDistributed, activations, constraints, initializers, regularizers, Recurrent\n",
    "from keras.layers.recurrent import _time_distributed_dense\n",
    "from keras.legacy import interfaces\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.constraints import Constraint\n",
    "from keras.layers.core import Dense\n",
    "from keras.engine import InputSpec\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import itertools\n",
    "\n",
    "'''\n",
    "Input: data is a matrix of shape (t,s) containing aggregate instantaneous power waveforms\n",
    "            [s being the number of samples per voltage-cycle, t number of cycles]\n",
    "            \n",
    "Output: A dictionary containing mean inter-event waveforms and their respective lengths\n",
    "\n",
    "'''\n",
    "def extract_events(data):\n",
    "    active_power = np.mean(data,axis=1)\n",
    "    delta = np.abs(active_power[1:] - active_power[:-1])\n",
    "    \n",
    "    w = 5\n",
    "    delta[delta<50] = 0\n",
    "    \n",
    "    votes = np.zeros(delta.shape)\n",
    "    for i in range(len(data)):\n",
    "        low = np.max([0,i-w])\n",
    "        high = np.min([i+w,len(delta)])\n",
    "        window = delta[low:high]\n",
    "        best = np.argmax(window)\n",
    "        if delta[low+best] > 0:\n",
    "            votes[low+best]+=1\n",
    "    \n",
    "    ev = [i for i,x in enumerate(votes) if x>3]\n",
    "    ev.append(len(data))\n",
    "    \n",
    "    low = 0\n",
    "    snippets_agg = []\n",
    "    snippets_length = []\n",
    "    \n",
    "    for e in ev:\n",
    "        snippets_length.append(e-low)\n",
    "        snippets_agg.append(np.mean(data[low+1:e],axis=0))\n",
    "        low = e\n",
    "        \n",
    "    ''' Uncomment this if you want to see a plot of what's going on\n",
    "    plt.plot(np.mean(data,axis=-1))\n",
    "    plt.plot(votes*100)\n",
    "    for e in ev:\n",
    "        plt.axvline(x=e, color='r')\n",
    "    '''\n",
    "        \n",
    "    return {'snippets_agg':np.array(snippets_agg), 'snippets_len':np.array(snippets_length)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a slight modification of the original $keras$ implementation of the LSTM. The only difference is that it outputs at time $t$ are $o(t)$ and $o(t-1)$. This allows for a much cleaner implementation of the variational loss, i.e. we don't have to deal with corner cases in the temporal mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_tm1(Recurrent):\n",
    "    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n",
    "    For a step-by-step description of the algorithm, see\n",
    "    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n",
    "    # Arguments\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](../activations.md)).\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        recurrent_activation: Activation function to use\n",
    "            for the recurrent step\n",
    "            (see [activations](../activations.md)).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "            used for the linear transformation of the inputs.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "            weights matrix,\n",
    "            used for the linear transformation of the recurrent state.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        unit_forget_bias: Boolean.\n",
    "            If True, add 1 to the bias of the forget gate at initialization.\n",
    "            Setting it to true will also force `bias_initializer=\"zeros\"`.\n",
    "            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        recurrent_regularizer: Regularizer function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its \"activation\").\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        recurrent_constraint: Constraint function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "        dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the inputs.\n",
    "        recurrent_dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the recurrent state.\n",
    "    # References\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "    \"\"\"\n",
    "    @interfaces.legacy_recurrent_support\n",
    "    def __init__(self, units,\n",
    "                 activation='hard_sigmoid',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 unit_forget_bias=True,\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 **kwargs):\n",
    "        super(LSTM_tm1, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        batch_size = input_shape[0] if self.stateful else None\n",
    "        self.input_dim = input_shape[2]\n",
    "        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n",
    "        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n",
    "                           InputSpec(shape=(batch_size, self.units))]\n",
    "\n",
    "        self.states = [None, None]\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "\n",
    "        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            (self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight((self.units * 4,),\n",
    "                                        name='bias',\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "            if self.unit_forget_bias:\n",
    "                bias_value = np.zeros((self.units * 4,))\n",
    "                bias_value[self.units: self.units * 2] = 1.\n",
    "                K.set_value(self.bias, bias_value)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.kernel_i = self.kernel[:, :self.units]\n",
    "        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n",
    "        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n",
    "        self.kernel_o = self.kernel[:, self.units * 3:]\n",
    "\n",
    "        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n",
    "        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n",
    "        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_i = self.bias[:self.units]\n",
    "            self.bias_f = self.bias[self.units: self.units * 2]\n",
    "            self.bias_c = self.bias[self.units * 2: self.units * 3]\n",
    "            self.bias_o = self.bias[self.units * 3:]\n",
    "        else:\n",
    "            self.bias_i = None\n",
    "            self.bias_f = None\n",
    "            self.bias_c = None\n",
    "            self.bias_o = None\n",
    "        self.built = True\n",
    "\n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        if self.implementation == 0:\n",
    "            input_shape = K.int_shape(inputs)\n",
    "            input_dim = input_shape[2]\n",
    "            timesteps = input_shape[1]\n",
    "\n",
    "            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n",
    "                                          self.dropout, input_dim, self.units,\n",
    "                                          timesteps, training=training)\n",
    "            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n",
    "                                          self.dropout, input_dim, self.units,\n",
    "                                          timesteps, training=training)\n",
    "            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n",
    "                                          self.dropout, input_dim, self.units,\n",
    "                                          timesteps, training=training)\n",
    "            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n",
    "                                          self.dropout, input_dim, self.units,\n",
    "                                          timesteps, training=training)\n",
    "            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def get_constants(self, inputs, training=None):\n",
    "        constants = []\n",
    "        if self.implementation == 0 and 0 < self.dropout < 1:\n",
    "            input_shape = K.int_shape(inputs)\n",
    "            input_dim = input_shape[-1]\n",
    "            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, int(input_dim)))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.dropout)\n",
    "\n",
    "            dp_mask = [K.in_train_phase(dropped_inputs,\n",
    "                                        ones,\n",
    "                                        training=training) for _ in range(4)]\n",
    "            constants.append(dp_mask)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "\n",
    "        if 0 < self.recurrent_dropout < 1:\n",
    "            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, self.units))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.recurrent_dropout)\n",
    "            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n",
    "                                            ones,\n",
    "                                            training=training) for _ in range(4)]\n",
    "            constants.append(rec_dp_mask)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "        return constants\n",
    "\n",
    "    def step(self, inputs, states):\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "        dp_mask = states[2]\n",
    "        rec_dp_mask = states[3]\n",
    "\n",
    "        if self.implementation == 2:\n",
    "            z = K.dot(inputs * dp_mask[0], self.kernel)\n",
    "            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n",
    "            if self.use_bias:\n",
    "                z = K.bias_add(z, self.bias)\n",
    "\n",
    "            z0 = z[:, :self.units]\n",
    "            z1 = z[:, self.units: 2 * self.units]\n",
    "            z2 = z[:, 2 * self.units: 3 * self.units]\n",
    "            z3 = z[:, 3 * self.units:]\n",
    "\n",
    "            i = self.recurrent_activation(z0)\n",
    "            f = self.recurrent_activation(z1)\n",
    "            c = f * c_tm1 + i * self.activation(z2)\n",
    "            o = self.activation(z3)\n",
    "        else:\n",
    "            if self.implementation == 0:\n",
    "                x_i = inputs[:, :self.units]\n",
    "                x_f = inputs[:, self.units: 2 * self.units]\n",
    "                x_c = inputs[:, 2 * self.units: 3 * self.units]\n",
    "                x_o = inputs[:, 3 * self.units:]\n",
    "            elif self.implementation == 1:\n",
    "                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n",
    "                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n",
    "                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n",
    "                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n",
    "            else:\n",
    "                raise ValueError('Unknown `implementation` mode.')\n",
    "\n",
    "            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n",
    "                                                      self.recurrent_kernel_i))\n",
    "            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n",
    "                                                      self.recurrent_kernel_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n",
    "                                                            self.recurrent_kernel_c))\n",
    "            o = self.activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n",
    "                                                      self.recurrent_kernel_o))\n",
    "        h = o * self.activation(c)\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            h._uses_learning_phase = True\n",
    "            \n",
    "        h_out = K.concatenate([h, h_tm1],axis=-1)\n",
    "        return h_out, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'unit_forget_bias': self.unit_forget_bias,\n",
    "                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'recurrent_dropout': self.recurrent_dropout}\n",
    "        base_config = super(LSTM_tm1, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a constraint that states that the instantaneous power of all inferred components must be strictly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class pinst_plus(Constraint):\n",
    "    \n",
    "    def __call__(self,p):\n",
    "        pinst = K.mean(p,axis=1)\n",
    "        pinst_offset = K.clip(pinst,-1000,0)\n",
    "        p = K.transpose(p)\n",
    "        p -= pinst_offset\n",
    "        p = K.transpose(p)\n",
    "        \n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational loss function\n",
    "The following code is where 'the magic happens'.\n",
    "\n",
    "Possible research paths:\n",
    "- Using a different distance measurement other than Euclidean distance for the difference loss would done by changing $fn\\_onerror$ or $fn\\_offerror$ and $noerror$\n",
    "- if you wanted to disaggregate 20+ components, then you would need to drop the sparsity constraint and create the matrix Z by sampling from probs. I have code somewhere that generates the k most probable configurations according to q, i.e. probs. Contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class var_loss:\n",
    "    \n",
    "    def __init__(self, layer, num_comps, time_delay, output_dim):\n",
    "        self.layer = layer #the dummy layer containing the component waveforms\n",
    "        self.num_comps = num_comps #number of inferred components\n",
    "        self.time_delay = time_delay #size of the minibatch\n",
    "        self.output_dim = output_dim #dimensionality of the aggregate and difference\n",
    "        \n",
    "        self.__name__ = 'var_loss'\n",
    "\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "\n",
    "        W = self.layer.layer.kernel  #component waveforms\n",
    "\n",
    "        dy_true = y_true[:,:,self.output_dim:] #desired diffference\n",
    "        y_true = y_true[:,:,:self.output_dim]  #desired aggregate\n",
    "\n",
    "        #all probabilities are clipped between EPS and 1-EPS to avoid log(0)\n",
    "        EPS = 0.00001\n",
    "\n",
    "        y_pred = K.clip(y_pred,EPS, 1-EPS)\n",
    "        probs = y_pred[:,:,0:self.num_comps] #sigma(t), i.e. component activation probabilities in Q\n",
    "        probs_tm1 = y_pred[:,:,self.num_comps:] #sigma(t-1)\n",
    "\n",
    "        \n",
    "        Z, Z_np = self.all_subsets(5, self.num_comps) #Z constitutes all admissable sparse configurations\n",
    "        \n",
    "        print('Building network')\n",
    "        \n",
    "        error_func = K.square #if you change this to K.abs, you will approximately use Laplace instead of Gaussians\n",
    "        zw = K.dot(Z,W) #the waveforms of all sparse configurations\n",
    "        \n",
    "        probs_tm1_nograd = tf.stop_gradient(probs_tm1) #for when the gradient shouldnt flow\n",
    "        \n",
    "        #next bloc computes losses associated with every component switching on or off at every time step\n",
    "        #in the temporal batch, as well as no component switching, i.e. O(t,i), I(t,i) and X(t,i) from paper\n",
    "        fn_onerror = lambda wi: -K.sum(error_func(dy_true - wi),axis=-1)\n",
    "        fn_offerror = lambda wi: -K.sum(error_func(dy_true + wi),axis=-1)\n",
    "        onerrors = tf.transpose(tf.map_fn(fn_onerror, W, parallel_iterations=1000),[1,2,0])\n",
    "        offerrors = tf.transpose(tf.map_fn(fn_offerror, W, parallel_iterations=1000),[1,2,0])\n",
    "        noerrors = -K.sum(error_func(dy_true),axis=-1)\n",
    "        \n",
    "        #in the following three tensors are computed: they are all of the shape [batch_size, t, num_z]\n",
    "        #num_z is the number of admissable sparse configurations\n",
    "        fn_rmse = lambda x: -K.sum(error_func(y_true - x),axis=-1)\n",
    "        fn_prob = lambda x: K.prod(probs*x + (1-probs)*(1-x),axis=-1)\n",
    "        fn_diff_tm1 = lambda z: K.sum(probs_tm1_nograd*(1-z)*offerrors + (1-probs_tm1_nograd)*z*onerrors,axis=-1) + \\\n",
    "                                K.prod(probs_tm1_nograd*z + (1-probs_tm1_nograd)*(1-z),axis=-1)*noerrors\n",
    "\n",
    "        alpha = 1.0\n",
    "        beta = 1.0\n",
    "        #these are the 3 matrices\n",
    "        Ls = tf.transpose(tf.map_fn(fn_rmse, zw, parallel_iterations=1000),[1,2,0]) #aggregate loss for every z\n",
    "        Ps = tf.transpose(tf.map_fn(fn_prob, Z, parallel_iterations=1000),[1,2,0]) #q(z|x) for every z\n",
    "        Ds = tf.transpose(tf.map_fn(fn_diff_tm1, Z, parallel_iterations=1000),[1,2,0]) #difference loss for every z\n",
    "\n",
    "        Ls = (Ds)*beta+Ls*alpha #combine losses\n",
    "        \n",
    "        #now we feed the loss into a softmax to obtain p(z|x)\n",
    "        e = K.exp(Ls - K.max(Ls, axis=-1, keepdims=True))\n",
    "        s = K.sum(e, axis=-1, keepdims=True)\n",
    "        Ls_sm = tf.stop_gradient(e/s) #p(z|x)\n",
    "        \n",
    "        y = K.dot(Ls_sm,Z) #this computes E[z] w.r.t. p(z|x) - simple dot product\n",
    "        E_pxz = K.sum(tf.stop_gradient(Ps)*Ls,axis=-1) #E[p(x,z)] w.r.t. q(z|x)\n",
    "        return (-K.sum(y*K.log(probs) + (1-y)*K.log(1-probs)) -E_pxz)/1000.0 #done\n",
    "\n",
    "\n",
    "    def all_subsets(self, num_act, num_c):\n",
    "        vals = np.eye(num_c)\n",
    "        Z = [np.zeros((num_c,))]\n",
    "        \n",
    "        for a in range(1,num_act+1):\n",
    "            for c in itertools.combinations(vals,a):\n",
    "                Z.append(np.sum(c,axis=0))\n",
    "                \n",
    "        return tf.constant(np.array(Z,dtype=np.float32)), np.array(Z,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "In the following the network topology is defined, as well as some helper function that help training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating net\n",
      "net created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(275, kernel_constraint=<__main__...., activation=\"linear\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(100, stateful=True, activation=\"tanh\", return_sequences=True, implementation=2)`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'InputSpec' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-11bd7ad41e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreset_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minput_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minput_scale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_delay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-11bd7ad41e8f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'net created'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mwf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwf_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_comps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_comps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwf0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minput_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-11bd7ad41e8f>\u001b[0m in \u001b[0;36mget_net\u001b[0;34m(input_dim, num_comps, time_delay, output_dim)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ml0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsume_less\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_tm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_comps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# modify the input spec to include the state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRecurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m    557\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a77dfffcb23c>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         self.kernel = self.add_weight((self.input_dim, self.units * 4),\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Layer must be stateful.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             raise ValueError('If a RNN is stateful, it needs to know '\n",
      "\u001b[0;31mTypeError\u001b[0m: 'InputSpec' object does not support indexing"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This creates the actual neural network. If you want to change the network topology. This is where you do it.\n",
    "'''\n",
    "\n",
    "def get_net(input_dim, num_comps, time_delay, output_dim):\n",
    "    \n",
    "    inp = Input(batch_shape=(batch_size,time_delay,input_dim), name='y_input')\n",
    "\n",
    "    lx = TimeDistributed(Dense(250, activation = 'relu'))(inp)\n",
    "    lz = TimeDistributed(Dense(250, activation = 'relu'))(lx)\n",
    "    bn = keras.layers.BatchNormalization()(lz)\n",
    "    ly = TimeDistributed(Dense(150, activation = 'tanh'))(bn)\n",
    "    \n",
    "    dummy_layer = TimeDistributed(Dense(num_comps), name='dummy')(ly)\n",
    "    recon = TimeDistributed(Dense(int(output_dim), activation = 'linear', W_constraint=pinst_plus()), name='waveforms')\n",
    "    reconed = recon(dummy_layer)\n",
    "    \n",
    "    l0 = LSTM(100, activation='tanh', return_sequences = True, consume_less='gpu', stateful=True)(ly)\n",
    "    l2 = LSTM_tm1(num_comps, activation='sigmoid', return_sequences = True, stateful=True)(l0)\n",
    "    \n",
    "    \n",
    "    model = Model(input=inp, output=[reconed, l2])\n",
    "    optimizer = RMSprop(lr=0.0001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,loss=['mse', var_loss(recon, num_comps, time_delay, output_dim)],\n",
    "                                            loss_weights = [0., 1.], sample_weight_mode='temporal')\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "Use this to train the network. It resets internal states of the network when needed\n",
    "It also takes the length of the different snippets into account (sample weights)\n",
    "Note that all weights are saved after each iteration\n",
    "'''\n",
    "import time\n",
    "def reset_train(net, input, output, sample_weights, t_batch, num_comps):\n",
    "    weights = []\n",
    "    errors = []\n",
    "    weights.append(net.get_weights())\n",
    "    sample_weights = np.expand_dims(sample_weights[1:],0)\n",
    "    \n",
    "    for i in range(200):\n",
    "        try:\n",
    "            try:\n",
    "                init_states = np.zeros((1,num_comps))\n",
    "                init_states[0,0] = 1\n",
    "                net.layers[-1].reset_states([init_states,init_states])\n",
    "                errors_in_batch = []\n",
    "                start = time.time()\n",
    "                for j in range(int(input.shape[1]/t_batch)):\n",
    "                    io = input[:,j*t_batch:(j+1)*t_batch,:]\n",
    "                    oo = [y[:,j*t_batch:(j+1)*t_batch,:] for y in output]\n",
    "                    sw = sample_weights[:,j*t_batch:(j+1)*t_batch]\n",
    "                    h = net.train_on_batch(io, oo, sample_weight=[np.ones((1,20)),sw])\n",
    "                    errors_in_batch.append(h[0])\n",
    "                    print('batch done', h[0])\n",
    "                errors.append(np.mean(errors_in_batch))\n",
    "                weights.append(net.get_weights())\n",
    "                print(errors)\n",
    "                print('done in', np.round(start - time.time()))\n",
    "                np.save('/tmp/weights_ongoing_np',weights)\n",
    "                np.save('/tmp/error_ongoing_np',errors)\n",
    "            except KeyboardInterrupt:\n",
    "                print('out')\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print('I messed up:',e)\n",
    "    return weights, errors\n",
    "\n",
    "\n",
    "'''\n",
    "Use this to predict component activations once the network is trained\n",
    "'''\n",
    "def reset_pred(net, input, t_batch, num_comps):\n",
    "    init_states = np.zeros((1,num_comps))\n",
    "    init_states[0,0] = 1\n",
    "    net.layers[-1].reset_states([init_states,init_states])\n",
    "    #net.reset_states()\n",
    "    outputs_in_batch = []\n",
    "    for j in range(int(input.shape[1]/t_batch)):\n",
    "        io = input[:,j*t_batch:(j+1)*t_batch,:]\n",
    "        h = net.predict_on_batch(io)\n",
    "        outputs_in_batch.append(h)\n",
    "        print('batch done')\n",
    "\n",
    "    probs = np.concatenate([x[1] for x in outputs_in_batch],axis=1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "prepare_data converts the snippets extracted into the required format\n",
    "Also makes the inputs zero-mean unit-variance so we don't\n",
    "'''\n",
    "def prepare_data(d):\n",
    "    \n",
    "    a = np.zeros((d.shape[0]-1, 2*d.shape[1]),dtype=np.float32)\n",
    "    b = np.zeros((d.shape[0]-1, 2*d.shape[1]),dtype=np.float32)\n",
    "    \n",
    "    l = d.shape[1]\n",
    "    \n",
    "    for i in range(len(d)-1):\n",
    "        a[i,:l] = d[i,:]\n",
    "        a[i,l:] = d[i+1,:] - d[i,:]\n",
    "        b[i,:l] = d[i+1,:]\n",
    "        b[i,l:] = d[i+1,:] - d[i,:]\n",
    "    \n",
    "    a = a-np.mean(a,axis=0)\n",
    "    a = a/np.sqrt(np.var(a,axis=0)+0.001)\n",
    "    \n",
    "    return a,b\n",
    "\n",
    "        \n",
    "\n",
    "time_delay = 20\n",
    "batch_size = 1\n",
    "\n",
    "'''\n",
    "Performs K-means clustering on the difference signal in order to initialize component waveforms\n",
    "'''\n",
    "def wf_init(b, num_comps, input_dim):\n",
    "    threshold = 10\n",
    "    diffs = np.mean(b[0,:,input_dim:],axis=-1)>threshold\n",
    "    c, _ = cluster.vq.kmeans(b[0,diffs,input_dim:], num_comps)\n",
    "    return c\n",
    "\n",
    "'''\n",
    "I recommend to pickle the dictionary create by the event detection algorithm.\n",
    "Here the pickle file is loaded and the data is brought into the correct format.\n",
    "'''\n",
    "\n",
    "def load_snippets():\n",
    "    d = pickle.load(open('redd31-events.pkl','rb'))\n",
    "    dat = d['snippets_agg']\n",
    "    a,b = prepare_data(dat)\n",
    "    a = np.expand_dims(a,axis=0)\n",
    "    b = np.expand_dims(b,axis=0)\n",
    "    \n",
    "    return a,b,d['snippets_len']\n",
    "\n",
    "'''\n",
    "Self explanatory: data is loaded, network created and then trained.\n",
    "'''\n",
    "def run():\n",
    "    input_dim = 275\n",
    "    input_scale = 500.0\n",
    "    num_comps = 10\n",
    "    a,b,l = load_snippets()\n",
    "    print('creating net')\n",
    "    print('net created')\n",
    "    wf0 = wf_init(b,num_comps, input_dim)\n",
    "    n = get_net(2*input_dim, num_comps, time_delay, input_dim)\n",
    "    w = n.get_weights()\n",
    "    w[15] = wf0/input_scale\n",
    "    n.set_weights(w)\n",
    "    print('starting to train')\n",
    "    w,e = reset_train(n, a, [b[:,:,:input_dim]/input_scale, \n",
    "                             b/input_scale], l, time_delay, num_comps)\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
